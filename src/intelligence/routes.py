# src/intelligence/routes.py - COMPLETE FIXED VERSION - NO INFINITE LOOPS
"""
Intelligence analysis routes - FIXED for Railway deployment
"""
from fastapi import APIRouter, Depends, HTTPException, status, UploadFile, File, Form
from sqlalchemy.ext.asyncio import AsyncSession
from sqlalchemy import select, func, and_
from pydantic import BaseModel, HttpUrl
from typing import List, Optional, Dict, Any
import uuid
from datetime import datetime
import traceback
import logging
import json

from src.core.database import get_db
from src.auth.dependencies import get_current_user
from src.models.user import User
from src.models.campaign import Campaign
from src.models.intelligence import (
    CampaignIntelligence, 
    GeneratedContent, 
    SmartURL, 
    IntelligenceSourceType,
    AnalysisStatus
)

# ‚úÖ FIXED: Safe import handling - No more crashes
try:
    from src.intelligence.analyzers import SalesPageAnalyzer, DocumentAnalyzer, WebAnalyzer, EnhancedSalesPageAnalyzer, VSLAnalyzer
    ANALYZERS_AVAILABLE = True
    logger.info("‚úÖ SUCCESS: All intelligence analyzers imported successfully")
except ImportError as e:
    logger.warning(f"‚ö†Ô∏è IMPORT WARNING: Analyzers not available: {str(e)}")
    ANALYZERS_AVAILABLE = False

try:
    from src.intelligence.generators import ContentGenerator, CampaignAngleGenerator
    GENERATORS_AVAILABLE = True
except ImportError as e:
    logger.warning(f"‚ö†Ô∏è IMPORT WARNING: Generators not available: {str(e)}")
    GENERATORS_AVAILABLE = False

try:
    from src.core.credits import check_and_consume_credits
    CREDITS_AVAILABLE = True
except ImportError:
    logger.warning("‚ö†Ô∏è WARNING: Credits system not available")
    CREDITS_AVAILABLE = False
    async def check_and_consume_credits(*args, **kwargs):
        pass

logger = logging.getLogger(__name__)
router = APIRouter(tags=["intelligence"])

# ============================================================================
# FALLBACK CLASSES FOR MISSING DEPENDENCIES
# ============================================================================

class FallbackAnalyzer:
    async def analyze(self, url: str) -> Dict[str, Any]:
        return {
            "offer_intelligence": {
                "products": ["Analysis requires missing dependencies"],
                "pricing": [],
                "bonuses": [],
                "guarantees": [],
                "value_propositions": ["Install aiohttp, beautifulsoup4, lxml to enable analysis"]
            },
            "psychology_intelligence": {
                "emotional_triggers": [],
                "pain_points": ["Dependency error - cannot analyze"],
                "target_audience": "Unknown",
                "persuasion_techniques": []
            },
            "competitive_intelligence": {
                "opportunities": ["Fix import dependencies to enable analysis"],
                "gaps": [],
                "positioning": "Analysis disabled",
                "advantages": [],
                "weaknesses": []
            },
            "content_intelligence": {
                "key_messages": [f"URL: {url}"],
                "success_stories": [],
                "social_proof": [],
                "content_structure": "Cannot analyze without dependencies"
            },
            "brand_intelligence": {
                "tone_voice": "Unknown",
                "messaging_style": "Unknown", 
                "brand_positioning": "Unknown"
            },
            "campaign_suggestions": [
                "Install missing dependencies: pip install aiohttp beautifulsoup4 lxml",
                "Check Railway deployment logs for import errors",
                "Verify requirements.txt contains all dependencies"
            ],
            "source_url": url,
            "page_title": "Analysis Failed - Missing Dependencies",
            "analysis_timestamp": datetime.utcnow().isoformat(),
            "confidence_score": 0.0,
            "raw_content": "",
            "error_message": "Missing dependencies: aiohttp, beautifulsoup4, lxml",
            "analysis_note": "Install required dependencies to enable URL analysis"
        }

class FallbackGenerator:
    async def generate_content(self, *args, **kwargs):
        return {
            "title": "Content Generation Disabled",
            "content": "Install missing dependencies to enable content generation",
            "metadata": {"error": "Missing dependencies"},
            "performance_predictions": {}
        }

# Use fallback if imports failed
if not ANALYZERS_AVAILABLE:
    SalesPageAnalyzer = FallbackAnalyzer
    DocumentAnalyzer = FallbackAnalyzer
    WebAnalyzer = FallbackAnalyzer
    EnhancedSalesPageAnalyzer = FallbackAnalyzer
    VSLAnalyzer = FallbackAnalyzer

if not GENERATORS_AVAILABLE:
    ContentGenerator = FallbackGenerator
    CampaignAngleGenerator = FallbackGenerator

# ============================================================================
# HELPER FUNCTIONS - CAMPAIGN COUNTER UPDATES
# ============================================================================

async def update_campaign_counters(campaign_id: str, db: AsyncSession):
    """Update campaign counter fields based on actual data"""
    try:
        # Count intelligence sources
        intelligence_count = await db.execute(
            select(func.count(CampaignIntelligence.id)).where(
                CampaignIntelligence.campaign_id == campaign_id
            )
        )
        sources_count = intelligence_count.scalar() or 0
        
        # Count generated content
        content_count = await db.execute(
            select(func.count(GeneratedContent.id)).where(
                GeneratedContent.campaign_id == campaign_id
            )
        )
        generated_content_count = content_count.scalar() or 0
        
        # Update campaign record
        from sqlalchemy import update
        await db.execute(
            update(Campaign).where(Campaign.id == campaign_id).values(
                sources_count=sources_count,
                intelligence_extracted=sources_count,
                intelligence_count=sources_count,
                content_generated=generated_content_count,
                generated_content_count=generated_content_count,
                updated_at=datetime.utcnow()
            )
        )
        
        logger.info(f"üìä Updated campaign counters: {sources_count} sources, {generated_content_count} content")
        return True
        
    except Exception as e:
        logger.error(f"‚ùå Error updating campaign counters: {str(e)}")
        return False

# ============================================================================
# REQUEST/RESPONSE MODELS
# ============================================================================

class AnalyzeURLRequest(BaseModel):
    url: HttpUrl
    campaign_id: str
    analysis_type: str = "sales_page"

class AnalysisResponse(BaseModel):
    intelligence_id: str
    analysis_status: str
    confidence_score: float
    offer_intelligence: Dict[str, Any]
    psychology_intelligence: Dict[str, Any]
    competitive_opportunities: List[Dict[str, Any]]
    campaign_suggestions: List[str]

class GenerateContentRequest(BaseModel):
    intelligence_id: str
    content_type: str
    preferences: Dict[str, Any] = {}
    campaign_id: str

class ContentGenerationResponse(BaseModel):
    content_id: str
    content_type: str
    generated_content: Dict[str, Any]
    smart_url: Optional[str] = None
    performance_predictions: Dict[str, Any]

# ============================================================================
# MAIN ANALYSIS ENDPOINT - FIXED
# ============================================================================

@router.post("/analyze-url", response_model=AnalysisResponse)
async def analyze_sales_page(
    request: AnalyzeURLRequest,
    current_user: User = Depends(get_current_user),
    db: AsyncSession = Depends(get_db)
):
    """‚úÖ FIXED: Analyze competitor sales page with proper error handling"""
    
    logger.info(f"üéØ Starting URL analysis for: {str(request.url)}")
    
    # Check credits if system is available
    if CREDITS_AVAILABLE:
        try:
            await check_and_consume_credits(
                user=current_user,
                operation="intelligence_analysis",
                credits_required=5,
                db=db
            )
        except Exception as e:
            logger.warning(f"‚ö†Ô∏è Credits check failed but continuing: {str(e)}")
    
    # Verify campaign ownership
    try:
        campaign_result = await db.execute(
            select(Campaign).where(
                and_(
                    Campaign.id == request.campaign_id,
                    Campaign.company_id == current_user.company_id
                )
            )
        )
        campaign = campaign_result.scalar_one_or_none()
        if not campaign:
            raise HTTPException(
                status_code=status.HTTP_404_NOT_FOUND,
                detail="Campaign not found"
            )
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"‚ùå Error verifying campaign: {str(e)}")
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail="Failed to verify campaign access"
        )
    
    # Create intelligence record
    try:
        intelligence = CampaignIntelligence(
            source_url=str(request.url),
            source_type=IntelligenceSourceType.SALES_PAGE,
            campaign_id=uuid.UUID(request.campaign_id),
            user_id=current_user.id,
            company_id=current_user.company_id,
            analysis_status=AnalysisStatus.PROCESSING
        )
        
        db.add(intelligence)
        await db.commit()
        await db.refresh(intelligence)
        
        logger.info(f"‚úÖ Created intelligence record: {intelligence.id}")
        
    except Exception as e:
        logger.error(f"‚ùå Error creating intelligence record: {str(e)}")
        await db.rollback()
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail="Failed to create intelligence record"
        )
    
    # Run analysis
    try:
        # Create analyzer
        if request.analysis_type == "sales_page":
            analyzer = SalesPageAnalyzer()
        elif request.analysis_type == "website":
            analyzer = WebAnalyzer()
        else:
            analyzer = SalesPageAnalyzer()
        
        logger.info(f"üîß Using analyzer: {type(analyzer).__name__}")
        
        # Run the analysis
        analysis_result = await analyzer.analyze(str(request.url))
        
        logger.info(f"üìà Analysis completed with confidence: {analysis_result.get('confidence_score', 0.0)}")
        
        # Update intelligence record with results
        intelligence.offer_intelligence = analysis_result.get("offer_intelligence", {})
        intelligence.psychology_intelligence = analysis_result.get("psychology_intelligence", {})
        intelligence.content_intelligence = analysis_result.get("content_intelligence", {})
        intelligence.competitive_intelligence = analysis_result.get("competitive_intelligence", {})
        intelligence.brand_intelligence = analysis_result.get("brand_intelligence", {})
        intelligence.confidence_score = analysis_result.get("confidence_score", 0.0)
        intelligence.source_title = analysis_result.get("page_title", "Analyzed Page")
        intelligence.raw_content = analysis_result.get("raw_content", "")
        
        # Set status based on whether we have real results or fallback
        if ANALYZERS_AVAILABLE and analysis_result.get("confidence_score", 0.0) > 0:
            intelligence.analysis_status = AnalysisStatus.COMPLETED
        else:
            intelligence.analysis_status = AnalysisStatus.FAILED
            intelligence.processing_metadata = {
                "error": analysis_result.get("error_message", "Dependencies missing"),
                "note": analysis_result.get("analysis_note", "Install aiohttp, beautifulsoup4, lxml")
            }
        
        await db.commit()
        
        # Update campaign counters (non-critical)
        try:
            await update_campaign_counters(request.campaign_id, db)
            await db.commit()
            logger.info(f"üìä Campaign counters updated")
        except Exception as counter_error:
            logger.warning(f"‚ö†Ô∏è Campaign counter update failed (non-critical): {str(counter_error)}")
        
        # Extract competitive opportunities
        competitive_intel = analysis_result.get("competitive_intelligence", {})
        competitive_opportunities = []
        if isinstance(competitive_intel.get("opportunities"), list):
            for opp in competitive_intel["opportunities"]:
                competitive_opportunities.append({"description": str(opp), "priority": "medium"})
        
        campaign_suggestions = analysis_result.get("campaign_suggestions", [])
        
        logger.info(f"‚úÖ Analysis completed successfully for: {str(request.url)}")
        
        return AnalysisResponse(
            intelligence_id=str(intelligence.id),
            analysis_status=intelligence.analysis_status.value,
            confidence_score=intelligence.confidence_score,
            offer_intelligence=intelligence.offer_intelligence,
            psychology_intelligence=intelligence.psychology_intelligence,
            competitive_opportunities=competitive_opportunities,
            campaign_suggestions=campaign_suggestions
        )
        
    except Exception as e:
        logger.error(f"‚ùå Analysis failed for {str(request.url)}: {str(e)}")
        
        # Update status to failed
        try:
            intelligence.analysis_status = AnalysisStatus.FAILED
            intelligence.processing_metadata = {
                "error": str(e),
                "traceback": traceback.format_exc()
            }
            await db.commit()
        except:
            await db.rollback()
        
        # Return a failed analysis instead of raising exception
        return AnalysisResponse(
            intelligence_id=str(intelligence.id),
            analysis_status="failed",
            confidence_score=0.0,
            offer_intelligence={"products": [], "pricing": [], "bonuses": [], "guarantees": [], "value_propositions": []},
            psychology_intelligence={"emotional_triggers": [], "pain_points": [], "target_audience": "Unknown", "persuasion_techniques": []},
            competitive_opportunities=[{"description": f"Analysis failed: {str(e)}", "priority": "high"}],
            campaign_suggestions=[
                "Check server logs for detailed error information",
                "Verify all dependencies are installed",
                "Try with a different URL"
            ]
        )

# ============================================================================
# ‚úÖ FIXED: CAMPAIGN INTELLIGENCE ENDPOINT - NO MORE INFINITE LOOPS
# ============================================================================

@router.get("/campaign/{campaign_id}/intelligence")
async def get_campaign_intelligence(
    campaign_id: str,
    current_user: User = Depends(get_current_user),
    db: AsyncSession = Depends(get_db)
):
    """‚úÖ FIXED: Get all intelligence sources for a campaign - Transaction Safe"""
    
    logger.info(f"üîç Getting intelligence for campaign: {campaign_id}")
    
    try:
        # ‚úÖ STEP 1: Verify campaign access (simple query, no joins)
        campaign_result = await db.execute(
            select(Campaign).where(
                and_(
                    Campaign.id == campaign_id,
                    Campaign.company_id == current_user.company_id
                )
            )
        )
        campaign = campaign_result.scalar_one_or_none()
        
        if not campaign:
            raise HTTPException(
                status_code=status.HTTP_404_NOT_FOUND,
                detail="Campaign not found"
            )
        
        logger.info(f"‚úÖ Campaign access verified: {campaign.title}")
        
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"‚ùå Error verifying campaign access: {str(e)}")
        await db.rollback()
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail="Failed to verify campaign access"
        )
    
    try:
        # ‚úÖ STEP 2: Get intelligence sources (safe query)
        intelligence_query = select(CampaignIntelligence).where(
            CampaignIntelligence.campaign_id == campaign_id
        ).order_by(CampaignIntelligence.created_at.desc())
        
        intelligence_result = await db.execute(intelligence_query)
        intelligence_sources = intelligence_result.scalars().all()
        
        logger.info(f"‚úÖ Found {len(intelligence_sources)} intelligence sources")
        
    except Exception as e:
        logger.error(f"‚ùå Error getting intelligence sources: {str(e)}")
        await db.rollback()
        # Return empty instead of failing
        intelligence_sources = []
    
    try:
        # ‚úÖ STEP 3: Get generated content (safe query)
        content_query = select(GeneratedContent).where(
            GeneratedContent.campaign_id == campaign_id
        ).order_by(GeneratedContent.created_at.desc())
        
        content_result = await db.execute(content_query)
        generated_content = content_result.scalars().all()
        
        logger.info(f"‚úÖ Found {len(generated_content)} generated content items")
        
    except Exception as e:
        logger.error(f"‚ùå Error getting generated content: {str(e)}")
        # Return empty instead of failing
        generated_content = []
    
    # ‚úÖ STEP 4: Build response safely (no database operations)
    try:
        # Calculate summary statistics
        total_intelligence = len(intelligence_sources)
        total_content = len(generated_content)
        avg_confidence = 0.0
        
        if intelligence_sources:
            confidence_scores = [
                source.confidence_score for source in intelligence_sources 
                if source.confidence_score is not None
            ]
            avg_confidence = sum(confidence_scores) / len(confidence_scores) if confidence_scores else 0.0
        
        # Convert to response format safely
        intelligence_data = []
        for source in intelligence_sources:
            try:
                intelligence_data.append({
                    "id": str(source.id),
                    "source_title": source.source_title or "Untitled Source",
                    "source_url": source.source_url or "",
                    "source_type": source.source_type.value if source.source_type else "unknown",
                    "confidence_score": source.confidence_score or 0.0,
                    "usage_count": source.usage_count or 0,
                    "analysis_status": source.analysis_status.value if source.analysis_status else "unknown",
                    "created_at": source.created_at.isoformat() if source.created_at else None,
                    "updated_at": source.updated_at.isoformat() if source.updated_at else None,
                    # Include intelligence data for frontend use
                    "offer_intelligence": source.offer_intelligence or {},
                    "psychology_intelligence": source.psychology_intelligence or {},
                    "content_intelligence": source.content_intelligence or {},
                    "competitive_intelligence": source.competitive_intelligence or {},
                    "brand_intelligence": source.brand_intelligence or {}
                })
            except Exception as source_error:
                logger.warning(f"‚ö†Ô∏è Error processing intelligence source {source.id}: {str(source_error)}")
                # Skip problematic sources instead of failing
                continue
        
        content_data = []
        for content in generated_content:
            try:
                content_data.append({
                    "id": str(content.id),
                    "content_type": content.content_type or "unknown",
                    "content_title": content.content_title or "Untitled Content",
                    "created_at": content.created_at.isoformat() if content.created_at else None,
                    "user_rating": content.user_rating,
                    "is_published": content.is_published or False,
                    "performance_data": content.performance_data or {}
                })
            except Exception as content_error:
                logger.warning(f"‚ö†Ô∏è Error processing content {content.id}: {str(content_error)}")
                # Skip problematic content instead of failing
                continue
        
        response = {
            "campaign_id": campaign_id,
            "intelligence_sources": intelligence_data,
            "generated_content": content_data,
            "summary": {
                "total_intelligence_sources": total_intelligence,
                "total_generated_content": total_content,
                "avg_confidence_score": round(avg_confidence, 3)
            }
        }
        
        logger.info(f"‚úÖ Intelligence response prepared successfully")
        
        return response
        
    except Exception as e:
        logger.error(f"‚ùå Error building response: {str(e)}")
        logger.error(f"üìç Response building traceback: {traceback.format_exc()}")
        
        # Return minimal response instead of failing
        return {
            "campaign_id": campaign_id,
            "intelligence_sources": [],
            "generated_content": [],
            "summary": {
                "total_intelligence_sources": 0,
                "total_generated_content": 0,
                "avg_confidence_score": 0.0
            },
            "error": "Failed to build complete response",
            "partial_data": True
        }

# ============================================================================
# CONTENT GENERATION ENDPOINT - FIXED
# ============================================================================

@router.post("/generate-content", response_model=ContentGenerationResponse)
async def generate_content_from_intelligence(
    request: GenerateContentRequest,
    current_user: User = Depends(get_current_user),
    db: AsyncSession = Depends(get_db)
):
    """‚úÖ FIXED: Generate marketing content from analyzed intelligence"""
    
    logger.info(f"üéØ Starting content generation: {request.content_type}")
    
    if not GENERATORS_AVAILABLE:
        raise HTTPException(
            status_code=status.HTTP_503_SERVICE_UNAVAILABLE,
            detail="Content generation is currently unavailable due to missing dependencies"
        )
    
    # Get intelligence data
    try:
        intelligence_result = await db.execute(
            select(CampaignIntelligence).where(
                and_(
                    CampaignIntelligence.id == request.intelligence_id,
                    CampaignIntelligence.company_id == current_user.company_id
                )
            )
        )
        intelligence = intelligence_result.scalar_one_or_none()
        
        if not intelligence:
            raise HTTPException(
                status_code=status.HTTP_404_NOT_FOUND,
                detail="Intelligence source not found"
            )
        
        logger.info(f"‚úÖ Found intelligence source: {intelligence.id}")
        
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"‚ùå Error finding intelligence: {str(e)}")
        await db.rollback()
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail="Failed to find intelligence source"
        )
    
    # Check credits
    if CREDITS_AVAILABLE:
        try:
            await check_and_consume_credits(
                user=current_user,
                operation="content_generation",
                credits_required=2,
                db=db
            )
            logger.info(f"‚úÖ Credits checked successfully")
        except Exception as e:
            logger.warning(f"‚ö†Ô∏è Credits check failed but continuing: {str(e)}")
    
    try:
        # Generate content
        logger.info(f"üîß Starting content generation...")
        
        generator = ContentGenerator()
        
        content_result = await generator.generate_content(
            intelligence_data={
                "offer_intelligence": intelligence.offer_intelligence or {},
                "psychology_intelligence": intelligence.psychology_intelligence or {},
                "content_intelligence": intelligence.content_intelligence or {},
                "competitive_intelligence": intelligence.competitive_intelligence or {},
                "brand_intelligence": intelligence.brand_intelligence or {}
            },
            content_type=request.content_type,
            preferences=request.preferences or {}
        )
        
        logger.info(f"‚úÖ Content generated: {content_result.get('title', 'Untitled')}")
        
        # Prepare content data safely
        content_body = content_result.get("content", {})
        if isinstance(content_body, dict):
            content_body_str = json.dumps(content_body)
        else:
            content_body_str = str(content_body)
        
        metadata = content_result.get("metadata", {})
        if not isinstance(metadata, dict):
            metadata = {}
        
        preferences = request.preferences or {}
        if not isinstance(preferences, dict):
            preferences = {}
        
        intelligence_used_data = {
            "intelligence_id": str(intelligence.id),
            "source_url": str(intelligence.source_url or ""),
            "confidence_score": float(intelligence.confidence_score or 0.0)
        }
        
        # Create and save content record
        generated_content = GeneratedContent(
            content_type=str(request.content_type),
            content_title=str(content_result.get("title", f"Generated {request.content_type}")),
            content_body=content_body_str,
            content_metadata=metadata,
            generation_settings=preferences,
            intelligence_used=intelligence_used_data,
            campaign_id=uuid.UUID(request.campaign_id),
            intelligence_source_id=intelligence.id,
            user_id=current_user.id,
            company_id=current_user.company_id
        )
        
        db.add(generated_content)
        await db.commit()
        await db.refresh(generated_content)
        
        logger.info(f"‚úÖ Content saved to database: {generated_content.id}")
        
        # Handle optional features (non-critical)
        smart_url = None
        if content_result.get("needs_tracking"):
            try:
                smart_url_record = SmartURL(
                    short_code=f"cf{uuid.uuid4().hex[:8]}",
                    original_url=content_result.get("target_url", ""),
                    tracking_url=f"https://track.campaignforge.co/cf{uuid.uuid4().hex[:8]}",
                    campaign_id=uuid.UUID(request.campaign_id),
                    generated_content_id=generated_content.id,
                    user_id=current_user.id,
                    company_id=current_user.company_id
                )
                
                db.add(smart_url_record)
                await db.commit()
                smart_url = smart_url_record.tracking_url
                logger.info(f"‚úÖ Smart URL created: {smart_url}")
                
            except Exception as smart_url_error:
                logger.warning(f"‚ö†Ô∏è Smart URL creation failed (non-critical): {str(smart_url_error)}")
        
        # Update usage count (non-critical)
        try:
            intelligence.usage_count = (intelligence.usage_count or 0) + 1
            await db.commit()
            logger.info(f"‚úÖ Intelligence usage count updated")
        except Exception as usage_error:
            logger.warning(f"‚ö†Ô∏è Usage count update failed (non-critical): {str(usage_error)}")
        
        # Update campaign counters (non-critical)
        try:
            await update_campaign_counters(request.campaign_id, db)
            await db.commit()
            logger.info(f"‚úÖ Campaign counters updated")
        except Exception as counter_error:
            logger.warning(f"‚ö†Ô∏è Campaign counter update failed (non-critical): {str(counter_error)}")
        
        logger.info(f"üéâ Content generation completed successfully!")
        
        return ContentGenerationResponse(
            content_id=str(generated_content.id),
            content_type=request.content_type,
            generated_content=content_result,
            smart_url=smart_url,
            performance_predictions=content_result.get("performance_predictions", {})
        )
        
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"‚ùå Content generation failed: {str(e)}")
        logger.error(f"üìç Full traceback: {traceback.format_exc()}")
        
        try:
            await db.rollback()
        except:
            pass
        
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail=f"Content generation failed: {str(e)}"
        )

# ============================================================================
# OTHER ENDPOINTS (SIMPLIFIED)
# ============================================================================

@router.post("/upload-document")
async def upload_document_for_analysis(
    file: UploadFile = File(...),
    campaign_id: str = Form(...),
    current_user: User = Depends(get_current_user),
    db: AsyncSession = Depends(get_db)
):
    """Upload and analyze documents"""
    
    if not ANALYZERS_AVAILABLE:
        raise HTTPException(
            status_code=status.HTTP_503_SERVICE_UNAVAILABLE,
            detail="Document analysis is currently unavailable due to missing dependencies"
        )
    
    # Basic file validation
    allowed_extensions = ["pdf", "docx", "txt", "pptx"]
    file_extension = file.filename.split('.')[-1].lower()
    
    if file_extension not in allowed_extensions:
        raise HTTPException(
            status_code=status.HTTP_400_BAD_REQUEST,
            detail=f"File type .{file_extension} not supported. Allowed: {', '.join(allowed_extensions)}"
        )
    
    try:
        # Read file content
        file_content = await file.read()
        
        # Create intelligence record
        intelligence = CampaignIntelligence(
            source_type=IntelligenceSourceType.DOCUMENT,
            source_title=file.filename,
            campaign_id=uuid.UUID(campaign_id),
            user_id=current_user.id,
            company_id=current_user.company_id,
            analysis_status=AnalysisStatus.PROCESSING
        )
        
        db.add(intelligence)
        await db.commit()
        await db.refresh(intelligence)
        
        # Analyze document
        analyzer = DocumentAnalyzer()
        analysis_result = await analyzer.analyze_document(file_content, file_extension)
        
        # Update intelligence with results
        intelligence.content_intelligence = analysis_result.get("content_intelligence", {})
        intelligence.competitive_intelligence = analysis_result.get("competitive_intelligence", {})
        intelligence.confidence_score = analysis_result.get("confidence_score", 0.7)
        intelligence.raw_content = analysis_result.get("extracted_text", "")
        intelligence.analysis_status = AnalysisStatus.COMPLETED
        
        await db.commit()
        
        # Update campaign counters (non-critical)
        try:
            await update_campaign_counters(campaign_id, db)
            await db.commit()
        except Exception as counter_error:
            logger.warning(f"‚ö†Ô∏è Campaign counter update failed: {str(counter_error)}")
        
        return {
            "intelligence_id": str(intelligence.id),
            "status": "completed",
            "insights_extracted": len(analysis_result.get("key_insights", [])),
            "content_opportunities": analysis_result.get("content_opportunities", [])
        }
        
    except Exception as e:
        logger.error(f"‚ùå Document analysis failed: {str(e)}")
        await db.rollback()
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail=f"Document analysis failed: {str(e)}"
        )

# ============================================================================
# CAMPAIGN COUNTER SYNC ENDPOINT
# ============================================================================

@router.post("/campaigns/{campaign_id}/sync-counters")
async def sync_campaign_counters(
    campaign_id: str,
    current_user: User = Depends(get_current_user),
    db: AsyncSession = Depends(get_db)
):
    """Manually sync campaign counters with actual data"""
    
    try:
        # Verify campaign ownership
        campaign_result = await db.execute(
            select(Campaign).where(
                and_(
                    Campaign.id == campaign_id,
                    Campaign.company_id == current_user.company_id
                )
            )
        )
        campaign = campaign_result.scalar_one_or_none()
        if not campaign:
            raise HTTPException(status_code=404, detail="Campaign not found")
        
        # Update counters
        success = await update_campaign_counters(campaign_id, db)
        if success:
            await db.commit()
        
        # Get updated campaign data
        updated_campaign_result = await db.execute(
            select(Campaign).where(Campaign.id == campaign_id)
        )
        updated_campaign = updated_campaign_result.scalar_one()
        
        return {
            "campaign_id": campaign_id,
            "sources_count": getattr(updated_campaign, 'sources_count', 0),
            "intelligence_count": getattr(updated_campaign, 'intelligence_count', 0),
            "content_count": getattr(updated_campaign, 'content_generated', 0),
            "message": "Campaign counters synchronized successfully"
        }
        
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"‚ùå Error syncing campaign counters: {str(e)}")
        await db.rollback()
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail=f"Failed to sync campaign counters: {str(e)}"
        )

# ============================================================================
# ENHANCED ANALYSIS ENDPOINTS (SIMPLIFIED FOR STABILITY)
# ============================================================================

@router.post("/analyze-sales-page-enhanced")
async def analyze_sales_page_enhanced(
    request: dict,
    current_user: User = Depends(get_current_user),
    db: AsyncSession = Depends(get_db)
):
    """Enhanced sales page analysis with comprehensive insights"""
    
    if not ANALYZERS_AVAILABLE:
        return {
            "error": "Enhanced analysis not available",
            "message": "Missing dependencies: aiohttp, beautifulsoup4, lxml",
            "fallback_available": True
        }
    
    # Use the standard analyze endpoint for now
    standard_request = AnalyzeURLRequest(
        url=request.get("url"),
        campaign_id=request.get("campaign_id"),
        analysis_type="sales_page"
    )
    
    return await analyze_sales_page(standard_request, current_user, db)

@router.post("/vsl-analysis")
async def analyze_video_sales_letter(
    request: dict,
    current_user: User = Depends(get_current_user),
    db: AsyncSession = Depends(get_db)
):
    """Video Sales Letter detection and analysis"""
    
    if not ANALYZERS_AVAILABLE:
        return {
            "has_video": False,
            "error": "VSL analysis not available",
            "message": "Missing dependencies for video analysis"
        }
    
    # Fallback to standard analysis for now
    standard_request = AnalyzeURLRequest(
        url=request.get("url"),
        campaign_id=request.get("campaign_id"),
        analysis_type="sales_page"
    )
    
    result = await analyze_sales_page(standard_request, current_user, db)
    
    # Add VSL-specific fields
    result_dict = result.dict() if hasattr(result, 'dict') else result
    result_dict.update({
        "has_video": False,
        "video_analysis": "VSL analysis requires additional dependencies",
        "transcript_available": False
    })
    
    return result_dict

@router.post("/generate-campaign-angles")
async def generate_campaign_angles(
    request: dict,
    current_user: User = Depends(get_current_user),
    db: AsyncSession = Depends(get_db)
):
    """Generate campaign angles from multiple intelligence sources"""
    
    if not GENERATORS_AVAILABLE:
        return {
            "primary_angle": "Campaign angle generation not available",
            "alternative_angles": [],
            "error": "Missing dependencies for angle generation"
        }
    
    try:
        campaign_id = request.get("campaign_id")
        intelligence_sources = request.get("intelligence_sources", [])
        
        if not intelligence_sources:
            return {
                "primary_angle": "No intelligence sources provided",
                "alternative_angles": [],
                "message": "Add intelligence sources first"
            }
        
        # Get intelligence data
        intelligence_data = []
        for source_id in intelligence_sources:
            try:
                intel_result = await db.execute(
                    select(CampaignIntelligence).where(
                        and_(
                            CampaignIntelligence.id == source_id,
                            CampaignIntelligence.company_id == current_user.company_id
                        )
                    )
                )
                intel = intel_result.scalar_one_or_none()
                if intel:
                    intelligence_data.append({
                        "offer_intelligence": intel.offer_intelligence or {},
                        "psychology_intelligence": intel.psychology_intelligence or {},
                        "competitive_intelligence": intel.competitive_intelligence or {}
                    })
            except Exception as e:
                logger.warning(f"‚ö†Ô∏è Failed to get intelligence {source_id}: {str(e)}")
                continue
        
        if not intelligence_data:
            return {
                "primary_angle": "No valid intelligence sources found",
                "alternative_angles": [],
                "message": "Verify intelligence source IDs"
            }
        
        # Generate angles using available data
        generator = CampaignAngleGenerator()
        angles_result = await generator.generate_angles(
            intelligence_data=intelligence_data,
            target_audience=request.get("target_audience"),
            industry=request.get("industry"),
            preferences=request
        )
        
        return angles_result
        
    except Exception as e:
        logger.error(f"‚ùå Campaign angle generation failed: {str(e)}")
        return {
            "primary_angle": f"Generation failed: {str(e)}",
            "alternative_angles": [],
            "error": str(e)
        }

@router.post("/consolidate/{campaign_id}")
async def consolidate_campaign_intelligence(
    campaign_id: str,
    request: dict,
    current_user: User = Depends(get_current_user),
    db: AsyncSession = Depends(get_db)
):
    """Consolidate multiple intelligence sources into unified insights"""
    
    try:
        # Get all intelligence for the campaign
        intelligence_query = select(CampaignIntelligence).where(
            and_(
                CampaignIntelligence.campaign_id == campaign_id,
                CampaignIntelligence.company_id == current_user.company_id,
                CampaignIntelligence.analysis_status == AnalysisStatus.COMPLETED
            )
        )
        
        intelligence_result = await db.execute(intelligence_query)
        intelligence_sources = intelligence_result.scalars().all()
        
        if not intelligence_sources:
            return {
                "campaign_id": campaign_id,
                "total_sources": 0,
                "message": "No completed intelligence sources found",
                "consolidated_insights": []
            }
        
        # Consolidate insights
        consolidated_insights = []
        confidence_scores = []
        
        for source in intelligence_sources:
            if source.confidence_score:
                confidence_scores.append(source.confidence_score)
            
            # Extract key insights from each source
            if source.offer_intelligence:
                offer_data = source.offer_intelligence
                if isinstance(offer_data, dict) and offer_data.get("value_propositions"):
                    consolidated_insights.extend(offer_data["value_propositions"])
            
            if source.psychology_intelligence:
                psych_data = source.psychology_intelligence
                if isinstance(psych_data, dict) and psych_data.get("emotional_triggers"):
                    consolidated_insights.extend(psych_data["emotional_triggers"])
        
        avg_confidence = sum(confidence_scores) / len(confidence_scores) if confidence_scores else 0.0
        
        # Remove duplicates and limit results
        unique_insights = list(set(consolidated_insights))[:10]
        
        return {
            "campaign_id": campaign_id,
            "total_sources": len(intelligence_sources),
            "confidence_weighted_score": round(avg_confidence, 3),
            "top_insights": unique_insights,
            "common_patterns": unique_insights[:5],  # Top 5 as common patterns
            "conflicting_insights": [],  # Would need more complex analysis
            "recommended_actions": [
                "Use top insights for content creation",
                "Test different value propositions",
                "Focus on emotional triggers in messaging"
            ]
        }
        
    except Exception as e:
        logger.error(f"‚ùå Intelligence consolidation failed: {str(e)}")
        return {
            "campaign_id": campaign_id,
            "error": str(e),
            "message": "Consolidation failed"
        }

@router.post("/batch-analyze")
async def batch_analyze_competitors(
    request: dict,
    current_user: User = Depends(get_current_user),
    db: AsyncSession = Depends(get_db)
):
    """Batch analyze multiple competitor URLs"""
    
    urls = request.get("urls", [])
    campaign_id = request.get("campaign_id")
    
    if not urls:
        raise HTTPException(
            status_code=status.HTTP_400_BAD_REQUEST,
            detail="No URLs provided for analysis"
        )
    
    if len(urls) > 10:  # Limit batch size
        raise HTTPException(
            status_code=status.HTTP_400_BAD_REQUEST,
            detail="Maximum 10 URLs allowed per batch"
        )
    
    results = []
    
    for url in urls:
        try:
            # Analyze each URL
            analysis_request = AnalyzeURLRequest(
                url=url,
                campaign_id=campaign_id,
                analysis_type="sales_page"
            )
            
            result = await analyze_sales_page(analysis_request, current_user, db)
            results.append({
                "url": url,
                "status": "completed",
                "intelligence_id": result.intelligence_id,
                "confidence_score": result.confidence_score
            })
            
        except Exception as e:
            logger.warning(f"‚ö†Ô∏è Failed to analyze {url}: {str(e)}")
            results.append({
                "url": url,
                "status": "failed",
                "error": str(e)
            })
    
    return {
        "campaign_id": campaign_id,
        "total_urls": len(urls),
        "successful_analyses": len([r for r in results if r["status"] == "completed"]),
        "failed_analyses": len([r for r in results if r["status"] == "failed"]),
        "results": results
    }

@router.post("/validate-url")
async def validate_and_pre_analyze_url(
    request: dict,
    current_user: User = Depends(get_current_user)
):
    """Validate URL and provide pre-analysis insights"""
    
    url = request.get("url")
    if not url:
        raise HTTPException(
            status_code=status.HTTP_400_BAD_REQUEST,
            detail="URL is required"
        )
    
    try:
        # Basic URL validation
        from urllib.parse import urlparse
        parsed = urlparse(url)
        
        is_valid = bool(parsed.netloc and parsed.scheme in ['http', 'https'])
        
        return {
            "is_valid": is_valid,
            "is_accessible": is_valid,  # Simplified check
            "page_type": "unknown",  # Would need actual page analysis
            "analysis_readiness": {
                "content_extractable": is_valid,
                "video_detected": False,
                "estimated_analysis_time": "30-60 seconds",
                "confidence_prediction": 0.7 if is_valid else 0.0
            },
            "optimization_suggestions": [
                "URL appears valid" if is_valid else "URL format invalid"
            ],
            "analysis_recommendations": {
                "recommended_analysis_type": "sales_page",
                "expected_insights": [
                    "Offer analysis",
                    "Psychology triggers",
                    "Competitive intelligence"
                ] if is_valid else [],
                "potential_limitations": [
                    "Requires stable internet connection",
                    "Some dynamic content may not be captured"
                ]
            }
        }
        
    except Exception as e:
        logger.error(f"‚ùå URL validation failed: {str(e)}")
        return {
            "is_valid": False,
            "is_accessible": False,
            "error": str(e)
        }